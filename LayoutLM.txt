LayoutLM Explained in Hindi:

Link - https://huggingface.co/transformers/v3.2.0/glossary.html#attention-mask

[Forward Pass]
LayoutLM ke forward pass me 5 input jate hai with different shapes which we gonna see:

{Input IDs} - ye tokens ki indices hoti hai jo as input use hoti hai by the model

Here’s an example using the BERT tokenizer, which is a WordPiece tokenizer:

>>> from transformers import BertTokenizer
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
>>> sequence = "A Titan RTX has 24GB of VRAM"
>>> tokenized_sequence = tokenizer.tokenize(sequence)

print(tokenized_sequence)
['A', 'Titan', 'R', '##T', '##X', 'has', '24', '##GB', 'of', 'V', '##RA', '##M']


Tokenizer mera convert kardeta sequence ko tokens me aur ye tokens words hote ya sub-wods hote 

---> Phir ye tokens IDs me convert kiye ja skte hai jise mera model samaj sakta hai aur ye kam seedhe sentence ko tokenizer me bhej ke ho jata hai

>>> inputs = tokenizer(sequence)

The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The token indices are under the key “input_ids”:

>>> encoded_sequence = inputs["input_ids"]
>>> print(encoded_sequence)
[101, 138, 18696, 155, 1942, 3190, 1144, 1572, 13745, 1104, 159, 9664, 2107, 102]

Note that the tokenizer automatically adds “special tokens” (if the associated model relies on them) which are special IDs the model sometimes uses.

------------------------------------------------------------------------------

[Attention mask]
ye optional argument hota hai. ye arrgument batata hai model ko ki kon-kon se tokens attend hone chahiye or kon-kon se nahi

For example, consider these two sequences:

>>> from transformers import BertTokenizer
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
>>> sequence_a = "This is a short sequence."
>>> sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."
>>> encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
>>> encoded_sequence_b = tokenizer(sequence_b)["input_ids"]

encoded_versions me different length hai 
>>> len(encoded_sequence_a), len(encoded_sequence_b)
(8, 19)

-> to isko hum ese pass nahi karskte hai. First sequence ko padd karna hoga dusre sequence ki length me lane ke liye ya phir second wale ko truncate kardo first ki length me

In the first case, the list of IDs will be extended by the padding indices. We can pass a list to the tokenizer and ask it to pad like this:
>>> padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)

--> to 0's add hogye hai on right of 1st sequence

>>> padded_sequences["input_ids"]
[[101, 1188, 1110, 170, 1603, 4954, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1188, 1110, 170, 1897, 1263, 4954, 119, 1135, 1110, 1120, 1655, 2039, 1190, 1103, 4954, 138, 119, 102]]

aur ab karo convert into pytorch tensor.Aur ye attention mask ek binary tensor hai jo position indicate karta hai padded indices ki taki model attend na kar paye unpe..Jese BertTokenizer me (1)- indicate karta
hai ki value to attend honi hai and (0)- attend nahi honi
This attention mask is in the dictionary returned by the tokenizer under the key “attention_mask”:

>>> padded_sequences["attention_mask"]
[[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]

[Token Type IDs]
Some models’ purpose is to do sequence classification or question answering. These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such:

>>> # [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]
We can use our tokenizer to automatically generate such a sentence by passing the two sequences to tokenizer as two arguments (and not a list, like before) like this:

>>> from transformers import BertTokenizer
>>> tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
>>> sequence_a = "HuggingFace is based in NYC"
>>> sequence_b = "Where is HuggingFace based?"
>>> encoded_dict = tokenizer(sequence_a, sequence_b)
>>> decoded = tokenizer.decode(encoded_dict["input_ids"])

which will return:

>>> print(decoded)
[CLS] HuggingFace is based in NYC [SEP] Where is HuggingFace based? [SEP]

This is enough for some models to understand where one sequence ends and where another begins. However, other models, such as BERT, also deploy token type IDs (also called segment IDs). They are represented as a binary mask identifying the two types of sequence in the model.

The tokenizer returns this mask as the “token_type_ids” entry:

>>> encoded_dict['token_type_ids']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]

-------------------------------------------------------------------------------------------

[Position IDs]

Contrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of each token. Therefore, the position IDs (position_ids) are used by the model to identify each token’s position in the list of tokens.

They are an optional parameter. If no position_ids is passed to the model, the IDs are automatically created as absolute positional embeddings.

Absolute positional embeddings are selected in the range [0, config.max_position_embeddings - 1]. Some models use other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.